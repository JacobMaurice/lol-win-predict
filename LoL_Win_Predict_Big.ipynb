{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac3fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dtale\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25187",
   "metadata": {},
   "source": [
    "# Get user PUUID using their Riot ID and Tagline\n",
    "The IDs chosen were from accessing the LoL leaderboards and using a random number generator from 1-1000 to select one person from each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e0d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rmx4KevIeySe_KyU0zbL4tSlB3-TEJv7rq1TnwdP7eqMQomFzJcgf9_QNvXLMDSTGwlBcs6IGwkhCw', 'QaNhg4o33-p-4iNV66Qruym2oj0KI2NSutcpbpfAeKkqyO8O7pE0kM5ppObtD220uAV49_DI6oKc-g', 'wNXNFMDN2pcXHK7uruuT5uSiZBKGsiuXLbBxt_iiUhcxyYzKD0hqTV5Ba4lJmUnjnXO_uo2qea0kDg', 'oTvLhuNxvAaecqftOUJ5-P2GGiXWmmkXbxmj-kDPHTenjwRqUQWuIjPu0OGW7Ucrqs9GR2T3VV-hxg', 'XaPR5pNeVfA5TjOcJlZUgbA4A-qh3vQTTPljvKQUjPTymMF2yB2FthiCuwgoVIPKg8pie4L67gBzaQ', 'oyTXQGNARDnGUreTHWuUw0gq2GQ8CvqSH7gkHE28Ur019cTYqEWBuZ1tZS2bIdcEpekFCNr3P24ZKg', 'T-LtTmI2PtGX3EJ65XwJnL1OoqNDoVA82vDbIg61VQrfK36nSdj-nS-mHwcIXaRe8nRdPJl7n0AOcA', '0m7V-AdBUCBR0IPX3X9uI8c7_lUu7Y2wZvHKY-1E_3lnc6B63vnYIiEVIQAs-O99tTheDmVrWEVe1A', 'ygS_CFnIxzk8rQsX1NMk2AA_xr3CxGd9qehpuIAM_9yeF_cbrGlGaZCQTWrS_XEmn14xSsU9oPqKiA', '2d2kRbLq5OvaWwOjdFeRyqjyQIihawpVqPiFouj6Y9JoXUR-4CRjHMr27S04ZLuXtR4c1dDpK-Eruw']\n"
     ]
    }
   ],
   "source": [
    "# Set Riot API key (EXPIRES AFTER 24 HOURS)\n",
    "api_key = 'RGAPI-dde6b16f-e586-4e88-952c-d3c63bb6593c'\n",
    "\n",
    "# Define the RiotID and Tagline\n",
    "riot_ids = ['The Baron Buff', 'yer so vayne', 'CNY14', 'seajay28', 'Ch4t Restricted', 'Polite is matter', 'Dexless', 'Scanning', 'Jayweii', 'Ppalli']\n",
    "taglines = ['Corp', 'NA1', 'NA1', 'NA1', 'NA1', 'NA1', 'NA1', '1213', 'Bae', '777']\n",
    "puuids = []\n",
    "\n",
    "for i in range(len(riot_ids)):\n",
    "    # Construct the summoner API URL\n",
    "    summoner_url = f'https://americas.api.riotgames.com/riot/account/v1/accounts/by-riot-id/{riot_ids[i]}/{taglines[i]}'\n",
    "\n",
    "    # Add API key to the headers\n",
    "    headers = {'X-Riot-Token': api_key}\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(summoner_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        summoner_data = response.json()\n",
    "        puuids.append(summoner_data['puuid'])\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "print(puuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4af16",
   "metadata": {},
   "source": [
    "# Get user match history (past 50 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9854f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request recent XXX matches from user using puuid\n",
    "# Maximum of 100 previous games\n",
    "number_of_matches = 50\n",
    "match_ids = []\n",
    "\n",
    "for puuid in puuids:\n",
    "    # Match Id API URL\n",
    "    match_ids_url = f'https://americas.api.riotgames.com/lol/match/v5/matches/by-puuid/{puuid}/ids?start=0&count={number_of_matches}'\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(match_ids_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        match_ids.extend(response.json())\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c56659",
   "metadata": {},
   "source": [
    "# Get match information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5400ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request info on matches based on match IDs\n",
    "match_info_list = []\n",
    "counter = 0\n",
    "\n",
    "for match_id in match_ids:\n",
    "    # Match info API URL\n",
    "    match_info_url = f'https://americas.api.riotgames.com/lol/match/v5/matches/{match_id}'\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(match_info_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        match_info_list.append(response.json())\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    if counter == 100:\n",
    "        # Reset counter\n",
    "        counter = 0\n",
    "        \n",
    "        # Need to wait 120 seconds because of request rate limitations\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b3460",
   "metadata": {},
   "source": [
    "# Extract desired user stats for every match using their PUUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f254055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 00:09:49,343 - INFO     - Executing shutdown...\n",
      "2024-03-20 00:09:49,348 - INFO     - Not running with the Werkzeug Server, exiting by searching gc for BaseWSGIServer\n"
     ]
    }
   ],
   "source": [
    "# teamId dictionary\n",
    "teamId_dict = {100: 'Blue', 200: 'Red'}\n",
    "\n",
    "# Stores match stats for last XXX games\n",
    "match_stats = {}\n",
    "challenge_stats = {}\n",
    "\n",
    "# Stores team stats for last XXX games\n",
    "objectives_blue_total = {}\n",
    "objectives_red_total = {}\n",
    "\n",
    "# Create dictionary containing the role the users in the match were playing\n",
    "role_dict = {}\n",
    "\n",
    "# Participants order in match history: Blue (Top, Jungle, Mid, Bot, Support), Red (Top, Jungle, Mid, Bot, Support)\n",
    "role_values = ['Top', 'Jungle', 'Mid', 'Bot', 'Support', 'Top', 'Jungle', 'Mid', 'Bot', 'Support']\n",
    "\n",
    "# Fill in dictionary\n",
    "for i in range(10):\n",
    "    role_dict[i] = role_values[i]\n",
    "\n",
    "\n",
    "for i in range(len(match_info_list)):\n",
    "    match_info = match_info_list[i]\n",
    "    match_id = match_ids[i]\n",
    "    \n",
    "    # Used to store team objective stats\n",
    "    objectives_blue = {}\n",
    "    objectives_red = {}\n",
    "    \n",
    "    # Isolate the data containing team objectives (total kills, baron, dragons, grubs/horde, rift herald, towers, inhibitors)\n",
    "    objectives_blue[match_id] = match_info['info']['teams'][0]['objectives']\n",
    "    objectives_red[match_id] = match_info['info']['teams'][1]['objectives']\n",
    "    \n",
    "    # Store the team stats for each game\n",
    "    objectives_blue_renamed = {}\n",
    "    objectives_red_renamed = {}\n",
    "    \n",
    "    # Creating a new dictionary restructuring objectives_blue and objectives_red\n",
    "    for key1, values1 in objectives_blue[match_id].items():\n",
    "        for key2, values2 in values1.items():\n",
    "            column_name = f'{key1}{key2.capitalize()}'\n",
    "            objectives_blue_renamed[column_name] = values2\n",
    "\n",
    "    for key1, values1 in objectives_red[match_id].items():\n",
    "        for key2, values2 in values1.items():\n",
    "            column_name = f'{key1}{key2.capitalize()}'\n",
    "            objectives_red_renamed[column_name] = values2\n",
    "    \n",
    "    # Adding if the team won the game or not to the dictionary\n",
    "    objectives_blue_renamed['win'] = match_info['info']['teams'][0]['win']\n",
    "    objectives_red_renamed['win'] = match_info['info']['teams'][1]['win']\n",
    "\n",
    "    # Appending the stats for the game to a new dictionary that uses match ids as keys\n",
    "    objectives_blue_total.update({match_id: objectives_blue_renamed})\n",
    "    objectives_red_total.update({match_id: objectives_red_renamed})\n",
    "    \n",
    "\n",
    "# Convert to DataFrame\n",
    "match_stats_df = pd.DataFrame.from_dict(match_stats, orient='index')\n",
    "challenge_stats_df = pd.DataFrame.from_dict(challenge_stats, orient='index')\n",
    "objectives_blue_df = pd.DataFrame.from_dict(objectives_blue_total, orient='index')\n",
    "objectives_red_df = pd.DataFrame.from_dict(objectives_red_total, orient='index')\n",
    "\n",
    "# Create dummy variables for inhibitors, hordes and dragons\n",
    "objectives_blue_df = pd.get_dummies(objectives_blue_df, columns=['inhibitorKills', 'dragonKills', 'hordeKills'], prefix=['inhibitorKills', 'dragonKills', 'hordeKills'])\n",
    "objectives_red_df = pd.get_dummies(objectives_red_df, columns=['inhibitorKills', 'dragonKills', 'hordeKills'], prefix=['inhibitorKills', 'dragonKills', 'hordeKills'])\n",
    "\n",
    "# Reposition win column for clarity\n",
    "col_pop = objectives_blue_df.pop('win')\n",
    "objectives_blue_df.insert(0, 'win', col_pop)\n",
    "\n",
    "col_pop = objectives_red_df.pop('win')\n",
    "objectives_red_df.insert(0, 'win', col_pop)\n",
    "\n",
    "#dtale.show(objectives_blue_df).open_browser()\n",
    "#dtale.show(objectives_red_df).open_browser()\n",
    "\n",
    "# Combine the two DataFrames\n",
    "objectives_data = pd.concat([objectives_blue_df, objectives_red_df], ignore_index = True)\n",
    "dtale.show(objectives_data).open_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed83418",
   "metadata": {},
   "source": [
    "# Selecting a Machine Learning Model\n",
    "\n",
    "## Using sklearn machine learning libraries\n",
    "### Supervised Learning Algorithms\n",
    "\n",
    "**The data and results seen below are from a set of 500 previous games which is an everchanging list. So, the results below will not always match the results from the code and should just be used to understand the thought process.**\n",
    "\n",
    "Since the data we are working with is labelled with wins and losses, it makes the most sense to use a supervised learning algorithm.\n",
    "\n",
    "Potential options include:\n",
    "- Logistic Regression\n",
    "- K Nearest Neighbour\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "\n",
    "The data used to train these models are binary/categorical that contain whether or not the blue team get the first baron, champion kill, dragon, horde, inhibitor, rift herald, and tower.\n",
    "\n",
    "It is worth mentioning that due to the nature of these variables, if they are TRUE for the blue team, they are FALSE for the red team. So, the categorical variables in objectives_red_df are the opposite of objectives_blue_df, making it unnecessary to model both data frames.\n",
    "\n",
    "**Strength of Association**\n",
    "\n",
    "We can use a chi2 contingency table to view the association between the features and the target variable 'win'.\n",
    "```\n",
    "            feature        chi2          pval\n",
    "0    inhibitorFirst  206.514115  7.914379e-47\n",
    "1        baronFirst  116.699649  3.340054e-27\n",
    "2     dragonKills_0   71.839575  2.334235e-17\n",
    "3        towerFirst   66.724524  3.122265e-16\n",
    "4     dragonKills_4   32.554770  1.158804e-08\n",
    "5     dragonKills_1   22.728502  1.865798e-06\n",
    "6       dragonFirst   22.412765  2.199076e-06\n",
    "7     championFirst   22.406803  2.205912e-06\n",
    "8     dragonKills_2   20.979788  4.641544e-06\n",
    "9   riftHeraldFirst   18.314750  1.872518e-05\n",
    "10     hordeKills_6   14.930831  1.115254e-04\n",
    "11    dragonKills_3   13.692772  2.152814e-04\n",
    "12     hordeKills_0   13.430207  2.476048e-04\n",
    "13    dragonKills_5    9.110863  2.540959e-03\n",
    "14       hordeFirst    7.957599  4.788590e-03\n",
    "15     hordeKills_1    2.051322  1.520741e-01\n",
    "16     hordeKills_5    1.531607  2.158714e-01\n",
    "17     hordeKills_2    0.640297  4.236033e-01\n",
    "18     hordeKills_3    0.491195  4.833947e-01\n",
    "19     hordeKills_4    0.385742  5.345460e-01\n",
    "20    dragonKills_6    0.001944  9.648352e-01\n",
    "```\n",
    "\n",
    "### Machine Learning Model Metrics\n",
    "Due to the nature of League of Legends, a player's win/loss ratio should be close to 50%, meaning the target variable \"win\" is well balanced. So, accuracy acts as a simple yet useful indicator of the effectiveness of our machine learning models and will be the primary metric taken into consideration when choosing a model. At the time of testing, the current win/loss ratio is 47.8% (239/500).\n",
    "\n",
    "Confusion Matrix = [[TN, FP], [FN, TP]]\n",
    "\n",
    "Accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "Recall = TP/(TP + FN)\n",
    "\n",
    "F1 score: weighted average of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9676cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract binary objective results\n",
    "X_cols = []\n",
    "for col in objectives_blue_df:\n",
    "    val_type = objectives_blue_df[col].dtypes\n",
    "    if val_type == bool and col != 'win':\n",
    "        X_cols.append(col)\n",
    "\n",
    "# Separating X and y\n",
    "X_data = objectives_blue_df[X_cols]\n",
    "y_data = objectives_blue_df['win']\n",
    "\n",
    "# Create a contingency table\n",
    "chi2_list = []\n",
    "pval_list = []\n",
    "\n",
    "# Checking the association between each X variable and the target variable\n",
    "for col in X_data:\n",
    "    contingency_table = pd.crosstab(X_data[col], y_data)\n",
    "    chi2_val, pval, _, _ = chi2_contingency(contingency_table)\n",
    "    chi2_list.append(chi2_val)\n",
    "    pval_list.append(pval)\n",
    "    \n",
    "chi2_df = pd.DataFrame({'feature': X_cols, 'chi2': chi2_list, 'pval': pval_list})\n",
    "\n",
    "chi2_df = chi2_df.sort_values(by = 'chi2', ascending = False).reset_index(drop=True)\n",
    "print(chi2_df)\n",
    "\n",
    "# Separate into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "print('\\n#===================== Simple Logistic Regression =====================#')\n",
    "# Create logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict if the game is a win or a loss on the test data\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Confidence the model has in each of its predictions\n",
    "y_pred_prob = lr.predict_proba(X_test)\n",
    "\n",
    "# Store summary of results in a new DataFrame\n",
    "y_results = pd.DataFrame(y_test)\n",
    "y_results['y_pred'] = y_pred\n",
    "y_results['y_loss_prob'] = y_pred_prob[:, 0]\n",
    "y_results['y_win_prob'] = y_pred_prob[:, 1]\n",
    "\n",
    "# View results\n",
    "y_results = pd.concat([y_results, X_test], axis=1)\n",
    "#dtale.show(y_results).open_browser()\n",
    "\n",
    "# Create confusion matrix comparing the predicted outcomes to the real outcomes\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mat)\n",
    "\n",
    "# Accuracy score\n",
    "lr_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nAccuracy Score: {lr_accuracy}')\n",
    "\n",
    "# Precision score\n",
    "lr_precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision Score: {lr_precision}')\n",
    "\n",
    "# Recall Score\n",
    "lr_recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall Score: {lr_recall}')\n",
    "\n",
    "# F1 Score\n",
    "lr_f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {lr_f1}')\n",
    "\n",
    "\n",
    "print('\\n#===================== K Nearest Neighbour =====================#')\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict if the game is a win or a loss on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Confidence the model has in each of its predictions\n",
    "y_pred_prob = knn.predict_proba(X_test)\n",
    "\n",
    "# Store summary of results in a new DataFrame\n",
    "y_results = pd.DataFrame(y_test)\n",
    "y_results['y_pred'] = y_pred\n",
    "y_results['y_loss_prob'] = y_pred_prob[:, 0]\n",
    "y_results['y_win_prob'] = y_pred_prob[:, 1]\n",
    "\n",
    "# View results\n",
    "y_results = pd.concat([y_results, X_test], axis=1)\n",
    "#dtale.show(y_results).open_browser()\n",
    "\n",
    "# Create confusion matrix comparing the predicted outcomes to the real outcomes\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mat)\n",
    "\n",
    "# Accuracy score\n",
    "knn_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nAccuracy Score: {knn_accuracy}')\n",
    "\n",
    "# Precision score\n",
    "knn_precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision Score: {knn_precision}')\n",
    "\n",
    "# Recall Score\n",
    "knn_recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall Score: {knn_recall}')\n",
    "\n",
    "# F1 Score\n",
    "knn_f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {knn_f1}')\n",
    "\n",
    "print('\\n#===================== Decision Tree Classifier =====================#')\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Fit training data\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict if the game is a win or a loss on the test data\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Confidence the model has in each of its predictions\n",
    "y_pred_prob = dtc.predict_proba(X_test)\n",
    "\n",
    "# Store summary of results in a new DataFrame\n",
    "y_results = pd.DataFrame(y_test)\n",
    "y_results['y_pred'] = y_pred\n",
    "y_results['y_loss_prob'] = y_pred_prob[:, 0]\n",
    "y_results['y_win_prob'] = y_pred_prob[:, 1]\n",
    "\n",
    "# View results\n",
    "y_results = pd.concat([y_results, X_test], axis=1)\n",
    "#dtale.show(y_results).open_browser()\n",
    "\n",
    "# Create confusion matrix comparing the predicted outcomes to the real outcomes\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mat)\n",
    "\n",
    "# Accuracy score\n",
    "dtc_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nAccuracy Score: {dtc_accuracy}')\n",
    "\n",
    "# Precision score\n",
    "dtc_precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision Score: {dtc_precision}')\n",
    "\n",
    "# Recall Score\n",
    "dtc_recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall Score: {dtc_recall}')\n",
    "\n",
    "# F1 Score\n",
    "dtc_f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {dtc_f1}')\n",
    "\n",
    "\n",
    "print('\\n#===================== Random Forest Classifier =====================#')\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict if the game is a win or a loss on the test data\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Confidence the model has in each of its predictions\n",
    "y_pred_prob = rfc.predict_proba(X_test)\n",
    "\n",
    "# Store summary of results in a new DataFrame\n",
    "y_results = pd.DataFrame(y_test)\n",
    "y_results['y_pred'] = y_pred\n",
    "y_results['y_loss_prob'] = y_pred_prob[:, 0]\n",
    "y_results['y_win_prob'] = y_pred_prob[:, 1]\n",
    "\n",
    "# View results\n",
    "y_results = pd.concat([y_results, X_test], axis=1)\n",
    "#dtale.show(y_results).open_browser()\n",
    "\n",
    "# Create confusion matrix comparing the predicted outcomes to the real outcomes\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mat)\n",
    "\n",
    "# Accuracy score\n",
    "rfc_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nAccuracy Score: {rfc_accuracy}')\n",
    "\n",
    "# Precision score\n",
    "rfc_precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision Score: {rfc_precision}')\n",
    "\n",
    "# Recall Score\n",
    "rfc_recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall Score: {rfc_recall}')\n",
    "\n",
    "# F1 Score\n",
    "rfc_f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {rfc_f1}')\n",
    "\n",
    "print('\\n#===================== Gradient Boosting Classifier =====================#')\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# Fit training data\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict if the game is a win or a loss on the test data\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Confidence the model has in each of its predictions\n",
    "y_pred_prob = gbc.predict_proba(X_test)\n",
    "\n",
    "# Store summary of results in a new DataFrame\n",
    "y_results = pd.DataFrame(y_test)\n",
    "y_results['y_pred'] = y_pred\n",
    "y_results['y_loss_prob'] = y_pred_prob[:, 0]\n",
    "y_results['y_win_prob'] = y_pred_prob[:, 1]\n",
    "\n",
    "# View results\n",
    "y_results = pd.concat([y_results, X_test], axis=1)\n",
    "#dtale.show(y_results).open_browser()\n",
    "\n",
    "# Create confusion matrix comparing the predicted outcomes to the real outcomes\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mat)\n",
    "\n",
    "# Accuracy score\n",
    "gbc_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nAccuracy Score: {gbc_accuracy}')\n",
    "\n",
    "# Precision score\n",
    "gbc_precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision Score: {gbc_precision}')\n",
    "\n",
    "# Recall Score\n",
    "gbc_recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall Score: {gbc_recall}')\n",
    "\n",
    "# F1 Score\n",
    "gbc_f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {gbc_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfc30e",
   "metadata": {},
   "source": [
    "# Incorporating K-Fold Cross-Validation\n",
    "\n",
    "This makes it so that every piece of data is used for training and testing and allows us to collect the mean score of every metric across various training/testing fits, making our results more reliable.\n",
    "\n",
    "**Results**\n",
    "\n",
    "After running the code several times, the accuracy of the logistic regression model consistently outperforms or is equal to the next best model, making it the model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract binary objective results\n",
    "X_cols = []\n",
    "for col in objectives_blue_df:\n",
    "    val_type = objectives_blue_df[col].dtypes\n",
    "    if val_type == bool and col != 'win':\n",
    "        X_cols.append(col)\n",
    "\n",
    "# Separating X and y\n",
    "X_data = objectives_blue_df[X_cols]\n",
    "y_data = objectives_blue_df['win']\n",
    "\n",
    "# Extract features and target as NumPy arrays\n",
    "X = X_data.values\n",
    "y = y_data.values\n",
    "\n",
    "# Initialize KFold\n",
    "# Use 5 folds to match the 80/20 train/test split of our previous tests\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Store score functions in dictionary for easier looping\n",
    "metric_functions = {'accuracy': accuracy_score, 'precision': precision_score, 'recall': recall_score, 'f1': f1_score}\n",
    "\n",
    "# Will store predicted target variables for each model\n",
    "model_pred = {}\n",
    "\n",
    "# Dictionary that will summarize all of the scores\n",
    "metrics = {'logisticRegression': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\\\n",
    "           'kNearestNeighbour': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\\\n",
    "           'decisionTreeClassifier': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\\\n",
    "           'randomForestClassifier': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\\\n",
    "           'gradientBoostingClassifier': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}}\n",
    "\n",
    "# Split the data using KFold and train on our models\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lr.fit(X_train, y_train)\n",
    "    model_pred['logisticRegression'] = lr.predict(X_test)\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    model_pred['kNearestNeighbour'] = knn.predict(X_test)\n",
    "    \n",
    "    dtc.fit(X_train, y_train)\n",
    "    model_pred['decisionTreeClassifier'] = dtc.predict(X_test)\n",
    "    \n",
    "    rfc.fit(X_train, y_train)\n",
    "    model_pred['randomForestClassifier'] = rfc.predict(X_test)\n",
    "    \n",
    "    gbc.fit(X_train, y_train)\n",
    "    model_pred['gradientBoostingClassifier'] = gbc.predict(X_test)\n",
    "    \n",
    "    # Add metrics to dictionary\n",
    "    # Loop through every model name\n",
    "    for model_name in metrics:\n",
    "        \n",
    "        # Loop through every metric function\n",
    "        for metric_name in metric_functions:\n",
    "            \n",
    "            # Predicted values for current model\n",
    "            y_pred = model_pred[model_name]\n",
    "            \n",
    "            # Apply corresponding score function\n",
    "            score = metric_functions[metric_name](y_test, y_pred)\n",
    "            \n",
    "            # Append score to list for corresponding model and metric\n",
    "            metrics[model_name][metric_name].append(score)\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "\n",
    "# Round the values to 2 decimal points\n",
    "metrics = metrics.applymap(lambda x: [round(num, 2) for num in x])\n",
    "\n",
    "# Add the mean of each score for each model\n",
    "metrics['lrMean'] = metrics.logisticRegression.apply(lambda row: np.mean(row))\n",
    "metrics['knnMean'] = metrics.kNearestNeighbour.apply(lambda row: np.mean(row))\n",
    "metrics['dtcMean'] = metrics.decisionTreeClassifier.apply(lambda row: np.mean(row))\n",
    "metrics['rfcMean'] = metrics.randomForestClassifier.apply(lambda row: np.mean(row))\n",
    "metrics['gbcMean'] = metrics.gradientBoostingClassifier.apply(lambda row: np.mean(row))\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "# After running the code several times, the accuracy of the logistic regression model consistently\n",
    "# outperforms or is equal to the next best model, making it the model of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5ebc4",
   "metadata": {},
   "source": [
    "# Logistic Regressor - Improving Model Performance\n",
    "## Looking at the confusion matrix\n",
    "\n",
    "By looking at the confusion matrix, we can see that there are consistently and noticeably more flase positives than false negatives. We may be able to improve our accuracy by changing the acceptance threshold of our model.\n",
    "\n",
    "It was determined that the optimal threshold is 0.55 with an accuracy of 0.88. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract binary objective results\n",
    "X_cols = []\n",
    "for col in objectives_blue_df:\n",
    "    val_type = objectives_blue_df[col].dtypes\n",
    "    if val_type == bool and col != 'win':\n",
    "        X_cols.append(col)\n",
    "\n",
    "# Separating X and y\n",
    "X_data = objectives_blue_df[X_cols]\n",
    "y_data = objectives_blue_df['win']\n",
    "\n",
    "# Separate into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Create logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict if the game is a win or a loss on the test data\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Confidence the model has in each of its predictions\n",
    "y_pred_prob = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create array of thresholds\n",
    "custom_thresholds = np.linspace(0, 1, 21)\n",
    "accuracies = []\n",
    "\n",
    "for threshold in custom_thresholds:\n",
    "    # y_pred that abides by the custom threshold\n",
    "    y_pred_custom = (y_pred_prob >= threshold)\n",
    "\n",
    "    # Evaluate the model using the custom threshold\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_custom))\n",
    "\n",
    "max_accuracy = max(accuracies)\n",
    "max_index = accuracies.index(max_accuracy)\n",
    "\n",
    "optimal_threshold = custom_thresholds[max_index]\n",
    "\n",
    "# Plotting accuracy for various acceptance thresholds\n",
    "ax = sns.lineplot(x = custom_thresholds, y = accuracies)\n",
    "ax.scatter(optimal_threshold, max_accuracy, color='red', label='Max Accuracy')\n",
    "\n",
    "# Set x and y labels\n",
    "ax.set(xlabel='Thresholds', ylabel='Accuracies')\n",
    "\n",
    "ax.annotate(f'{optimal_threshold}, {max_accuracy}', (optimal_threshold, max_accuracy))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85559664",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "Now that we have determined an optimal acceptance threshold, we can tune the hyperparamter C the inverse of regularization stregth. **It has been determined that the default value of 1 is ideal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03cc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define hyperparameter grid for grid search\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Create a grid search object with cross-validation\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "# Confidence model has in each of its predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_optimal = (y_pred_prob >= optimal_threshold)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy on the test set: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
